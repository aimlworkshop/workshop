{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reinforcement Learning Workshop\n",
    "In this workshop, we are going to train a model to find the safest path from the start to end locations on a frozen lake."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "import seaborn as sns\n",
    "from collections import defaultdict\n",
    "import copy \n",
    "import matplotlib.animation as animation\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Create a heatmap of the Q values per state.\n",
    "\"\"\"\n",
    "def createHeatMap(q_value_snapshots, snapshot_intervals, env):\n",
    "    cols = 2  # Number of columns\n",
    "    rows = 3  # Fixed to 2 rows\n",
    "    all_q_values = np.concatenate([\n",
    "        np.array([list(snapshot.get(state, np.zeros(env.action_space.n))) for state in range(env.observation_space.n)])\n",
    "        for snapshot in q_value_snapshots\n",
    "    ])\n",
    "    vmin, vmax = np.min(all_q_values), np.max(all_q_values)\n",
    "\n",
    "    # Create subplots with 2 rows\n",
    "    fig, axes = plt.subplots(rows, cols, figsize=(10, 15))  # Adjust height to fit better\n",
    "    axes = axes.flatten()  # Flatten axes for easier indexing\n",
    "\n",
    "    for i, q_snapshot in enumerate(q_value_snapshots):\n",
    "        q_table_array = np.zeros((env.observation_space.n, env.action_space.n))\n",
    "        \n",
    "        for state in range(env.observation_space.n):\n",
    "            q_table_array[state] = q_snapshot.get(state, np.zeros(env.action_space.n))  # Ensure all states exist\n",
    "\n",
    "        sns.heatmap(q_table_array, annot=True, cmap=\"coolwarm\", cbar=True, ax=axes[i], vmin=vmin, vmax=vmax)\n",
    "        axes[i].set_title(f\"Q-values at Episode {snapshot_intervals[i]}\")\n",
    "        axes[i].set_xlabel(\"Actions (0=Left, 1=Down, 2=Right, 3=Up)\")\n",
    "        axes[i].set_ylabel(\"States\")\n",
    "\n",
    "    # Hide any unused subplots (if len(q_value_snapshots) is odd)\n",
    "    for j in range(i + 1, len(axes)):\n",
    "        axes[j].axis(\"off\")\n",
    "\n",
    "    plt.suptitle(\"Q-value Evolution Over Training\", fontsize=16)\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.95])  # Adjust layout to fit title\n",
    "    plt.show()\n",
    "    createHeatMapAnimation(q_value_snapshots, snapshot_intervals, env)\n",
    "\n",
    "\"\"\"\n",
    "Creates an animation of the heat map (created above) evolving over episodes.\n",
    "\"\"\"\n",
    "def createHeatMapAnimation(q_value_snapshots, snapshot_intervals, env):\n",
    "    fig, ax = plt.subplots(figsize=(6, 5))\n",
    "\n",
    "    # Find global min and max values for consistent color scaling\n",
    "    all_q_values = np.concatenate([\n",
    "        np.array([list(snapshot.get(state, np.zeros(env.action_space.n))) for state in range(env.observation_space.n)])\n",
    "        for snapshot in q_value_snapshots\n",
    "    ])\n",
    "\n",
    "    vmin, vmax = np.min(all_q_values), np.max(all_q_values)  # Fixed color range\n",
    "\n",
    "    # Initialize first Q-table\n",
    "    q_table_array = np.zeros((env.observation_space.n, env.action_space.n))\n",
    "\n",
    "    # Create the initial heatmap with fixed vmin, vmax\n",
    "    heatmap = sns.heatmap(q_table_array, annot=True, cmap=\"coolwarm\", cbar=True, ax=ax, vmin=vmin, vmax=vmax)\n",
    "    cbar = heatmap.collections[0].colorbar  # Store the colorbar reference\n",
    "\n",
    "    def update(frame):\n",
    "        \"\"\"Update the heatmap for each frame without changing the color scale.\"\"\"\n",
    "        q_snapshot = q_value_snapshots[frame]\n",
    "\n",
    "        # Update Q-table array with the new snapshot\n",
    "        for state in range(env.observation_space.n):\n",
    "            q_table_array[state] = q_snapshot.get(state, np.zeros(env.action_space.n))  # Ensure all states exist\n",
    "\n",
    "        # Clear old heatmap but keep the color scale\n",
    "        ax.clear()\n",
    "\n",
    "        # Redraw heatmap with consistent color scale\n",
    "        sns.heatmap(q_table_array, annot=True, cmap=\"coolwarm\", cbar=False, ax=ax, vmin=vmin, vmax=vmax)\n",
    "\n",
    "        ax.set_title(f\"Q-values at Episode {snapshot_intervals[frame]}\")\n",
    "        ax.set_xlabel(\"Actions (0=Left, 1=Down, 2=Right, 3=Up)\")\n",
    "        ax.set_ylabel(\"States\")\n",
    "\n",
    "    # Create the animation\n",
    "    anim = animation.FuncAnimation(fig, update, frames=len(q_value_snapshots), repeat=True)\n",
    "\n",
    "    # Save as GIF\n",
    "    gif_filename = \"q_table_animation.gif\"\n",
    "    anim.save(gif_filename, writer=animation.PillowWriter(fps=1))\n",
    "    print(f\"✅ Animation saved as {gif_filename}\")\n",
    "\n",
    "\"\"\"\n",
    "Creates a heatmap of the number of times the model visits each state. \n",
    "\"\"\"\n",
    "def visualizeStateVisits(state_visits, env):\n",
    "    visit_grid = np.zeros((4, 4))\n",
    "    \n",
    "    for state, visits in state_visits.items():\n",
    "        visit_grid[state // 4, state % 4] = visits\n",
    "\n",
    "    plt.figure(figsize=(5, 5))\n",
    "    sns.heatmap(visit_grid, annot=True, cmap=\"Greens\", fmt=\".0f\")\n",
    "    plt.title(\"State Visit Frequency\")\n",
    "    plt.show()\n",
    "    return visit_grid\n",
    "\n",
    "\"\"\" \n",
    "Plots the smoothed reward over episodes.\n",
    "\"\"\"\n",
    "def plotRewards(reward_history, window_size=100):\n",
    "    \"\"\" Line plot of total rewards over episodes with smoothing. \"\"\"\n",
    "    smoothed_rewards = np.convolve(reward_history, np.ones(window_size)/window_size, mode='valid')\n",
    "\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(smoothed_rewards, label=f\"Smoothed Rewards (window={window_size})\", color=\"blue\", linewidth=2)\n",
    "    plt.xlabel(\"Episodes\")\n",
    "    plt.ylabel(\"Average Reward\")\n",
    "    plt.title(\"Training Progress - Smoothed Rewards Over Time\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    return smoothed_rewards\n",
    "\n",
    "\"\"\"\n",
    "Plots the best direction for each state.\n",
    "\"\"\"\n",
    "def getBestActionsPerState(env, Q, ACTION_MAP):\n",
    "    policy_grid = np.full((4, 4), ' ', dtype=str)\n",
    "    for state in range(env.observation_space.n):\n",
    "        best_action = np.argmax(Q[state])\n",
    "        policy_grid[state // 4, state % 4] = ACTION_MAP[best_action]\n",
    "\n",
    "    print(\"\\nBest Actions Learned by Q-Learning:\")\n",
    "    for row in policy_grid:\n",
    "        print(\"  \".join(row))\n",
    "\n",
    "\"\"\"\n",
    "    Function to compare variables across multiple categories in a dictionary,\n",
    "    creating subplots for each key in the dictionary.\n",
    "    \n",
    "    :param data_dict: Dictionary where each key has a list of variables to plot.\n",
    "    \"\"\"\n",
    "def compareAll(data_dict):    \n",
    "    # Number of rows = number of keys in the dictionary\n",
    "    num_rows = len(data_dict)\n",
    "    \n",
    "    # Number of columns = length of the longest list of values in the dictionary\n",
    "    num_cols = max(len(values) for values in data_dict.values())\n",
    "    \n",
    "    # Create a figure and axis grid\n",
    "    fig, axes = plt.subplots(num_rows, num_cols, figsize=(num_cols*5, num_rows*5))\n",
    "    \n",
    "    # Ensure axes is 2D array for easier indexing\n",
    "    if num_rows == 1:\n",
    "        axes = axes.reshape(1, num_cols)\n",
    "    if num_cols == 1:\n",
    "        axes = axes.reshape(num_rows, 1)\n",
    "    \n",
    "    # Loop through the dictionary and plot\n",
    "    for i, (key, values) in enumerate(data_dict.items()):\n",
    "        # Loop through the values list of the current key and plot them\n",
    "        for j, value in enumerate(values):\n",
    "            ax = axes[i, j]\n",
    "            if j == 0:\n",
    "                sns.heatmap(value, ax=ax, annot=True, cmap=\"Greens\", fmt=\".0f\")\n",
    "            if j == 1:\n",
    "                ax.plot(value)\n",
    "            \n",
    "            ax.set_title(f'{key} - Plot {j+1}')\n",
    "            ax.set_xlabel('X-axis')\n",
    "            ax.set_ylabel('Y-axis')\n",
    "            \n",
    "    # Adjust layout to prevent overlapping subplots\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\"\"\"\n",
    "Reinforcement learning model definition.\n",
    "\"\"\"\n",
    "def reinforcement_learning(alpha=0.1, gamma=0.9, epsilon=1.0, epsilon_decay=0.999, epsilon_min=0.1, episodes=5001, render_mode=\"ansi\", render=False):\n",
    "    ACTION_MAP = {0: '←', 1: '↓', 2: '→', 3: '↑'}\n",
    "    env = gym.make(\"FrozenLake-v1\", is_slippery=False, render_mode=render_mode)\n",
    "    Q = defaultdict(lambda: np.zeros(env.action_space.n))\n",
    "    \n",
    "    q_value_snapshots = []  # Store Q-table at different intervals\n",
    "    snapshot_intervals = np.linspace(0, episodes, 5, dtype=int).tolist()  # Capture Q-values at these episodes\n",
    "    state_visits = defaultdict(int)  # Track state visit frequency\n",
    "    reward_history = []  # Track total rewards per episode\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    for episode in range(episodes):\n",
    "        print(episode)\n",
    "        state, _ = env.reset()\n",
    "        done = False\n",
    "        total_reward = 0  # Track cumulative reward for this episode\n",
    "\n",
    "        while not done:\n",
    "\n",
    "            state_visits[state] += 1  # 🔹 Track visits\n",
    "            \n",
    "            if np.random.rand() < epsilon:\n",
    "                action = env.action_space.sample()\n",
    "            else:\n",
    "                action = np.argmax(Q[state])\n",
    "\n",
    "            next_state, reward, done, _, _ = env.step(action)\n",
    "            best_next_action = np.argmax(Q[next_state])\n",
    "            Q[state][action] += alpha * (reward + gamma * Q[next_state][best_next_action] - Q[state][action])\n",
    "            state = next_state\n",
    "\n",
    "            total_reward += reward  # 🔹 Accumulate reward\n",
    "\n",
    "        epsilon = max(epsilon * epsilon_decay, epsilon_min)  # Reduce exploration over time\n",
    "        reward_history.append(total_reward)\n",
    "\n",
    "        # 🔹 Store **actual copies** of Q-values at specific intervals\n",
    "        if episode in snapshot_intervals:\n",
    "            q_value_snapshots.append(copy.deepcopy(dict(Q)))  \n",
    "    end_time = time.time()  # End time of the epoch\n",
    "    final_time = end_time - start_time  # Store epoch time\n",
    "    print(\"Reinforcement Learning - Q-Learning Training Complete\")\n",
    "\n",
    "    print(\"\\nFrozenLake Map:\")\n",
    "    if (render):\n",
    "        print(env.render())\n",
    "    return env, Q, ACTION_MAP, q_value_snapshots, snapshot_intervals, state_visits, reward_history, final_time\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Functions to compare changes in hyperparameter values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def compare_alpha_values(alpha_values, episodes=5001):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    times = []\n",
    "    for alpha in alpha_values:\n",
    "        print(f\"Training with alpha = {alpha}\")\n",
    "        _, _, _, _, _, _, reward_history, finalTime = reinforcement_learning(alpha=alpha, episodes=episodes)\n",
    "        times.append(finalTime)\n",
    "        smoothed_rewards = np.convolve(reward_history, np.ones(100)/100, mode='same')\n",
    "        plt.plot(smoothed_rewards, label=f'α = {alpha}')\n",
    "\n",
    "    plt.title(\"Comparison of Performance with Different Alpha Values\")\n",
    "    plt.xlabel(\"Episodes\")\n",
    "    plt.ylabel(\"Cumulative Reward\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    createPlot(alpha_values, times, \"Alpha values\", \"Times\", \"Latency For Each Alpha Value\")\n",
    "\n",
    "def compare_gamma_values(gamma_values, episodes=5001):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    times = []\n",
    "    for gamma in gamma_values:\n",
    "        print(f\"Training with gamma = {gamma}\")\n",
    "        _, _, _, _, _, _, reward_history, finalTime = reinforcement_learning(gamma=gamma, episodes=episodes)\n",
    "        times.append(finalTime)  # Store epoch time\n",
    "        smoothed_rewards = np.convolve(reward_history, np.ones(100)/100, mode='same')\n",
    "        plt.plot(smoothed_rewards, label=f'gamma = {gamma}')\n",
    "\n",
    "    plt.title(\"Comparison of Performance with Different Gamma Values\")\n",
    "    plt.xlabel(\"Episodes\")\n",
    "    plt.ylabel(\"Cumulative Reward\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    createPlot(gamma_values, times, \"Gamma values\", \"Times\", \"Latency For Each Gamma Value\")\n",
    "\n",
    "def compare_epsilon_values(epsilon_values, episodes=5001):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    times = []\n",
    "    for epsilon in epsilon_values:\n",
    "        print(f\"Training with epsilon = {epsilon}\")\n",
    "        _, _, _, _, _, _, reward_history, finalTime = reinforcement_learning(epsilon=epsilon, episodes=episodes)\n",
    "        times.append(finalTime)\n",
    "        smoothed_rewards = np.convolve(reward_history, np.ones(100)/100, mode='same')\n",
    "        plt.plot(smoothed_rewards, label=f'epsilon = {epsilon}')\n",
    "\n",
    "    plt.title(\"Comparison of Performance with Different Epsilon Values\")\n",
    "    plt.xlabel(\"Episodes\")\n",
    "    plt.ylabel(\"Cumulative Reward\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    createPlot(epsilon_values, times, \"Epsilon values\", \"Times\", \"Latency For Each Epsilon Value\")\n",
    "\n",
    "def compare_epsilon_decay_values(epsilon_decay_values, episodes=5001):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    times = []\n",
    "    for epsilon_decay in epsilon_decay_values:\n",
    "        print(f\"Training with epsilon_decay = {epsilon_decay}\")\n",
    "        _, _, _, _, _, _, reward_history, finalTime = reinforcement_learning(epsilon_decay=epsilon_decay, episodes=episodes)\n",
    "        times.append(finalTime)  # Store epoch time\n",
    "        smoothed_rewards = np.convolve(reward_history, np.ones(100)/100, mode='same')\n",
    "        plt.plot(smoothed_rewards, label=f'Decay = {epsilon_decay}')\n",
    "\n",
    "    plt.title(\"Comparison of Performance with Different Epsilon Decay Values\")\n",
    "    plt.xlabel(\"Episodes\")\n",
    "    plt.ylabel(\"Cumulative Reward\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    createPlot(epsilon_decay_values, times, \"Epsilon Decay values\", \"Times\", \"Latency For Each Epsilon Decay Value\")\n",
    "    \n",
    "\n",
    "\"\"\"\n",
    "Helper function to create plots.\n",
    "\"\"\"\n",
    "def createPlot(x, y, xlabel, ylabel, title):\n",
    "    plt.plot(x, y)\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lets Visualize The Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here you will see how the model trains using reinforcement learning. In your own words, can you describe how reinforcement learning works? What are episodes? What marks the end of an episode? Do you think 10 episodes is enough?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env, Q, ACTION_MAP, q_value_snapshots, snapshot_intervals, state_visits, reward_history, _ = reinforcement_learning(render_mode=\"human\", render=True, episodes=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analyze the model's performace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each cube on the map is referred to as a state starting with the top left as 0 and incrementing the state number as we move right and downwards. As a result, state 16 is the bottom right cube."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Description of each metric:\n",
    "\n",
    "Best Actions: What direction the model recommends taking when at that location on the frozen lake\n",
    "\n",
    "State Visit Frequency: The amount of times the model visited each state during the training process\n",
    "\n",
    "Q-values Evolution: Shows the evolution of what direction the model recommends at each state. The direction with the highest value is what the model recommends the most, second highest value is second recommended direction, etc.\n",
    "\n",
    "Smoothed Rewards: Every episode that the model successfuly gets to the goal, it gets a +1. If it doesn't get to the end it a 0. The smoothed rewards is the average of the previous `window_size` runs (in this case 1). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Shows the best action to take at each state.\n",
    "getBestActionsPerState(env, Q, ACTION_MAP)\n",
    "\n",
    "### See the number of times the model visited each state\n",
    "visualizeStateVisits(state_visits, env)\n",
    "\n",
    "### See the model learning which action is best at each state\n",
    "_ = createHeatMap(q_value_snapshots, snapshot_intervals, env)\n",
    "\n",
    "### Showing the rewards\n",
    "_ = plotRewards(reward_history, window_size=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How did the model perform? Why? How should we adjust the model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env, Q, ACTION_MAP, q_value_snapshots, snapshot_intervals, state_visits, reward_history, _ = reinforcement_learning()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add functions to see model performance. Hint: How did we analyze the first model? \n",
    "\n",
    "Note: To understand why smoothing rewards is important, compare the window_size = 1 vs window_size = 100 graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Try Changing Hyperparameter Values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hint: Use these functions: compare_alpha_values, compare_gamma_values, compare_epsilon_values, compare_epsilon_decay_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Current alpha value: 0.1\n",
    "alpha_values = ### Your code\n",
    "compare_alpha_values(alpha_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Current gamma value: 0.9\n",
    "gamma_values = ### Your code\n",
    "compare_gamma_values(gamma_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Current epsilon value: 1.0\n",
    "epsilon_values = ### Your code\n",
    "compare_epsilon_values(epsilon_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Current epsilon decay value: 0.999\n",
    "epsilon_decay_values = ### Your code\n",
    "compare_epsilon_decay_values(epsilon_decay_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How does each hyperparamater affect the model's performace? Which ones would you tune further?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
